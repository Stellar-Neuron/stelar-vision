<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose STELAR-Vision, a training framework explicitly trained for topology-aware reasoning.">
  <meta property="og:title" content="STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision"/>
  <meta property="og:description" content="We propose STELAR-Vision, a training framework explicitly trained for topology-aware reasoning."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/methodpipe.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision">
  <meta name="twitter:description" content="We propose STELAR-Vision, a training framework explicitly trained for topology-aware reasoning.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/methodpipe.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Large Language Models, Vision Language Models, Topology, Topology-aware reasoning, Reinforcement Learning from Human Feedback, LLM, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">STELAR-VISION</h1>
            <h2 class="subtitle is-3 publication-subtitle">Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Chen Li</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Han Zhang</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Zhantao Yang</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Fangyi Chen</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Zihan Wang</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Anudeepsekhar Bolimera</a>,</span>
              <span class="author-block"><a href="https://stellar-neuron.github.io/stelar-vision/" target="_blank">Marios Savvides</a></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/PLACEHOLDER.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (Coming soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/..." target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/PLACEHOLDER" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming soon)</span>
                </a>
              </span>

              <!-- HuggingFace Dataset Link -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/Stellar-Neuron/stelar-vision-collection-689a909010eff40d1f884d08" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fa fa-database"></i>
                </span>
                <span>HF Datasets</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss.
On MATH-V and VLM_S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks. The data and code will be available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Large image -->
<section class="section hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="hero-body">
        <img src="static/images/methodpipe.png" alt="MY ALT TEXT" class="large-image"/>
        <h2 class="subtitle has-text-centered">
          An overview of the STELAR-Vision framework.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End large image -->


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- Motivation image -->
<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Motivation</h2>
      </div>
      <img src="static/images/FigureOne.png" alt="MY ALT TEXT" class="primary-image"/>
      <h2 class="subtitle">
        <b>Limitations of the Popular Chain-of-Thought Reasoning Structures.</b> The widely adopted Chain-of-Thought (CoT) reasoning paradigm (in green) often results in unnecessarily verbose reasoning processes, as demonstrated in the first example. Under CoT reasoning, the model redundantly counts each cube, whereas with Graph topology (in blue), it quickly identifies the key point of the question. In the bottom-row example, CoT reasoning begins with a detailed examination of each subplot but ultimately arrives at an incorrect answer. In contrast, Tree topology (in red) initiates reasoning with a high-level overview before delving into specific features. In both scenarios, CoT-style reasoning proves suboptimal.
      </h2>
    </div>
    </div>
  </div>
</section>
<!-- End motivation image -->


<!-- Two images side by side -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-half has-text-centered">
        <img src="static/images/topo_accuracy_per_subject.png" alt="Comparison of topology accuracy across subjects" class="image"/>
        <h2 class="subtitle has-text-centered">
          <b>Comparison of topology accuracy across subjects:</b> Accuracy of Chain, Tree, and Graph reasoning topological structures per subject of MATH-V dataset. Chain remains the best overall reasoning structure, while Tree, and Graph perform better in at reasoning subjects such as "graph theory" or "statistics".
        </h2>
      </div>
      <div class="column is-half has-text-centered">
        <img src="static/images/violin_plot.png" alt="Caption B" class="image"/>
        <h2 class="subtitle has-text-centered">
          <b>Distribution of generated reasoning token length</b> of Chain, Tree, and Graph topological structures in TopoAug Dataset. The box within each violin plot represents the median, and 25% and 75% percentile thresholds.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End two images side by side -->


<!-- Paper contributions -->
<section class="section hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We propose STELAR-Vision, a training framework explicitly trained for topology-aware reasoning. It leverages diverse reasoning topologies such as chains, trees, and graphs, aligns reasoning paths with question characteristics, and enables adaptive and efficient multimodal inference.</li>
            <li>We introduce TopoAug, a data generation pipeline that automatically produces diverse topological reasoning and annotates optimal structures per question. We also integrate  Frugal Learning into the learning framework, achieving reductions in output length with minimal accuracy tradeoff.</li>
            <li>By conducting experiments with post-training supervision and reinforcement learning, STELAR-Vision improves accuracy by 9.7% over its base model and its larger variant Qwen2VL-72B-Instruct by 7.3%. On the out-of-distribution dataset, it surpasses The Frugal Learning variant reduces output length by 18.1% while maintaining comparable accuracy. </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper contributions -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item" style="translate: 0% 40%;">
        <img src="static/images/main_table.png" alt="Quantitative evaluation results of STELAR-Vision across benchmarks"/>
        <h2 class="subtitle has-text-centered">
          <strong>Quantitative Evaluation.</strong> STELAR-Vision achieves strong gains across both in-distribution and out-of-distribution reasoning benchmarks. On ID datasets, it outperforms its base model Qwen2VL-7B-Instruct by <strong>9.7%</strong>, and even surpasses the larger Qwen2VL-72B-Instruct by <strong>7.3%</strong>. On OOD benchmarks, it exceeds Phi-4-multimodal-instruct by up to <strong>36%</strong> and LLaMA-3.2-11B-Vision-Instruct by up to <strong>13.2%</strong>. Compared to Chain-Only training, STELAR-Vision achieves up to <strong>13%</strong> higher accuracy, highlighting the power of topological augmentation.
        </h2>
      </div>
      <div class="item" style="translate: 0% 30%;">
        <img src="static/images/train_method.png" alt="Impact of TopoAug Dataset and Training Methods"/>
        <h2 class="subtitle has-text-centered">
          <strong>Impact of TopoAug Dataset and Training Methods.</strong> We present an ablation study on the in-distribution VLM_S2H and Math-V datasets to compare the performance of our models against counterparts trained exclusively on chain-based reasoning data across all training methods. <strong>STELAR-Vision</strong> consistently outperforms all Chain-Only variants across all ID datasets-specifically it improves the highest variant Chain-Only from 25% to 31% by <strong>6%</strong>, and boosts overall accuracy by <strong>4.3%</strong>, highlighting the effectiveness of topological augmentation.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/token_len.png" alt="Comparison of accuracy and generated token length across models"/>
        <h2 class="subtitle has-text-centered">
          <strong>Comparison of accuracy and generated token length across models:</strong> STELAR-Vision improves performance while using fewer generation tokens. Frugal learning further improves generation efficiency.
       </h2>
     </div>
     <div class="item" style="translate: 0% 10%;">
      <img src="static/images/topo_accuracy_per_subject.png" alt="Comparison of topology accuracy across subjects in the MATH-V dataset"/>
      <h2 class="subtitle has-text-centered">
        <strong>Comparison of topology accuracy across subjects:</strong> Accuracy of Chain, Tree, and Graph reasoning topological structures per subject of the MATH-V dataset. Chain remains the best overall reasoning structure, while Tree and Graph perform better at reasoning subjects such as "graph theory" or "statistics".
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->

<section class="hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Experiments</h2>
      </div>

      <img src="static/images/main_table.png" alt="MY ALT TEXT" class="motivation-image"/>
      <h2 class="subtitle">
        <b>Quantitative Evaluation.</b> STELAR-Vision achieves strong gains across both in-distribution and out-of-distribution reasoning benchmarks. On ID datasets, it outperforms its base model Qwen2VL-7B-Instruct by <strong>9.7%</strong>, and even surpasses the larger Qwen2VL-72B-Instruct by <strong>7.3%</strong>. On OOD benchmarks, it exceeds Phi-4-multimodal-instruct by up to <strong>36%</strong> and LLaMA-3.2-11B-Vision-Instruct by up to <strong>13.2%</strong>. Compared to Chain-Only training, STELAR-Vision achieves up to <strong>13%</strong> higher accuracy, highlighting the power of topological augmentation.
      </h2>

      <br>

      <img src="static/images/train_method.png" alt="MY ALT TEXT" class="motivation-image"/>
      <h2 class="subtitle">
        <b>Impact of TopoAug Dataset and Training Methods.</b> We present an ablation study on the in-distribution VLM_S2H and Math-V datasets to compare the performance of our models against counterparts trained exclusively on chain-based reasoning data across all training methods. <strong>STELAR-Vision</strong> consistently outperforms all Chain-Only variants across all ID datasets-specifically it improves the highest variant Chain-Only from 25% to 31% by <strong>6%</strong>, and boosts overall accuracy by <strong>4.3%</strong>, highlighting the effectiveness of topological augmentation.
      </h2>

      <br>

      <img src="static/images/token_len.png" alt="MY ALT TEXT" class="motivation-image"/>
      <h2 class="subtitle">
        <b>Comparison of accuracy and generated token length across models:</b> STELAR-Vision improves performance while using fewer generation tokens. Frugal learning further improves generation efficiency.
      </h2>

    </div>
    </div>
  </div>
</section>


<!-- Paper contributions -->
<section class="section hero is-light is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we propose STELAR-Vision, a training framework that enables topology-aware reasoning in VLMs via generated responses. STELAR-Vision enhances vision-language reasoning by leveraging diverse topological structures, achieving a 9.7% accuracy improvement over its base model and outperforming its larger variant Qwen2VL-72B-Instruct by 7.3%. The Frugal Learning variant reduces output length by 18.1% while maintaining comparable accuracy, surpassing Chain-Only baselines in both efficiency and task effectiveness. STELAR-Vision demonstrates strong generalization across five diverse OOD datasets and achieves 4.3% higher overall accuracy on in-distribution tasks, consistently outperforming Chain-Only training.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper contributions -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming soon</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
